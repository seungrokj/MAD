[
  {
    "name": "pyt_huggingface_gpt2",
    "url": "https://github.com/huggingface/transformers",
    "dockerfile": "docker/pyt_huggingface",
    "scripts": "scripts/huggingface_gpt2/run.sh",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "fp16",
    "tags": [
      "pyt",
      "fp16",
      "gpt2"
    ],
    "args": ""
  },
  {
    "name": "pyt_huggingface_bert",
    "url": "https://github.com/huggingface/transformers",
    "dockerfile": "docker/pyt_huggingface",
    "scripts": "scripts/huggingface_bert/run.sh",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "bert"
    ],
    "args": ""
  },
  {
    "name": "pyt_vllm_llama-3.1-8b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo meta-llama/Meta-Llama-3.1-8B-Instruct --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_llama-3.1-70b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo meta-llama/Meta-Llama-3.1-70B-Instruct --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_llama-3.1-405b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo meta-llama/Meta-Llama-3.1-405B-Instruct --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_llama-2-7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo meta-llama/Llama-2-7b-chat-hf --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_llama-2-70b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo meta-llama/Llama-2-70b-chat-hf --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_mixtral-8x7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo mistralai/Mixtral-8x7B-Instruct-v0.1 --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_mixtral-8x22b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo mistralai/Mixtral-8x22B-Instruct-v0.1 --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_mistral-7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo mistralai/Mistral-7B-Instruct-v0.3 --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_qwen2-7b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo Qwen/Qwen2-7B-Instruct --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_qwen2-72b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo Qwen/Qwen2-72B-Instruct --test_option latency,throughput --num_gpu 8 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_jais-13b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo core42/jais-13b-chat --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_jais-30b",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo core42/jais-30b-chat-v3 --test_option latency,throughput --num_gpu 1 --datatype float16 --tunableop on"
  },
  {
    "name": "pyt_vllm_llama-3.1-8b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo amd/Meta-Llama-3.1-8B-Instruct-FP8-KV --test_option latency,throughput --num_gpu 1 --datatype float8 --tunableop on"
  },
  {
    "name": "pyt_vllm_llama-3.1-70b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo amd/Meta-Llama-3.1-70B-Instruct-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop on"
  },
  {
    "name": "pyt_vllm_llama-3.1-405b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo amd/Meta-Llama-3.1-405B-Instruct-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop on"
  },
  {
    "name": "pyt_vllm_mixtral-8x7b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo amd/Mixtral-8x7B-Instruct-v0.1-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop on"
  },
  {
    "name": "pyt_vllm_mixtral-8x22b_fp8",
    "url": "",
    "dockerfile": "docker/pyt_vllm",
    "scripts": "scripts/vllm/run.sh",
    "data": "huggingface",
    "n_gpus": "-1",
    "owner": "mad.support@amd.com",
    "training_precision": "",
    "tags": [
      "pyt",
      "vllm"
    ],
    "timeout": 28800,
    "args":
     "--model_repo amd/Mixtral-8x22B-Instruct-v0.1-FP8-KV --test_option latency,throughput --num_gpu 8 --datatype float8 --tunableop on"
  }
]
